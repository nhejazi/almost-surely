---
author: "Nima Hejazi"
categories: [ "R", "data science", "machine learning", "computing" ]
tags: [ "R", "data science", "machine learning", "computing" ]
date: "2018-01-02"
description: "Simplifying machine learning in R through pipelines"
featured: ""
featuredalt: ""
featuredpath: ""
linktitle: ""
title: "sl3: Machine Learning Pipelines for R"
type: "post"
#comments: false
disableComments: true
published: false
output:
  blogdown::html_page:
    toc: false

---



<!--
IN PROGRESS:
* https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html
* https://github.com/jeremyrcoyle/sl3/issues/104
-->
<p><strong>This post is a working draft and is published only to share with coauthors.</strong></p>
<p>Common in the language of modern data science are words such as “munging,” “massaging,” “mining” – all words denoting the interactive process by which the analyst extracts some form of deliverable inference from a given data set. These terms express, among other things, the (often) convoluted process by which a set of pre-processing and estimation procedures are applied to an input data set in order to transform said data set into a <a href="http://vita.had.co.nz/papers/tidy-data.html">“tidy”</a> output data set from which informative visualizations and summaries may be easily extracted. A formalism that captures this involved process is that of machine learning <em>pipelines</em>. A <em>pipeline</em>, popularized by the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">method of the same name</a> in Python’s <a href="http://scikit-learn.org/stable/index.html">scikit-learn library</a>, may be thought of as a simple bundle that documents procedures to be applied to as input data set in a particular order, ultimately resulting in a tidy output data set.</p>
<p>Recently, the pipeline idiom has made its way into the R programming language, via the new <a href="https://github.com/jeremyrcoyle/sl3"><code>sl3</code> R package</a>. A concrete understanding of the utility of pipelines is best developed by example – so, that’s precisely what we’ll do! In the following, we’ll apply the concept of a machine learning pipeline to the canonical <a href="">iris data set</a>, combining a series of learners (machine learning algorithms for estimation/classification) with Principal Components analysis, a simple pre-processing step.</p>
<pre class="r"><code>library(datasets)
library(tidyverse)
library(data.table)
library(caret)
library(sl3)
set.seed(352)</code></pre>
<p>…</p>
<pre class="r"><code>data(iris)
iris &lt;- iris %&gt;%
  as_tibble(.)
iris</code></pre>
<pre><code>## # A tibble: 150 x 5
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
##  1         5.10        3.50         1.40       0.200 setosa 
##  2         4.90        3.00         1.40       0.200 setosa 
##  3         4.70        3.20         1.30       0.200 setosa 
##  4         4.60        3.10         1.50       0.200 setosa 
##  5         5.00        3.60         1.40       0.200 setosa 
##  6         5.40        3.90         1.70       0.400 setosa 
##  7         4.60        3.40         1.40       0.300 setosa 
##  8         5.00        3.40         1.50       0.200 setosa 
##  9         4.40        2.90         1.40       0.200 setosa 
## 10         4.90        3.10         1.50       0.100 setosa 
## # ... with 140 more rows</code></pre>
<p>…</p>
<p>To create very simple training and testing splits, we’ll rely on the popular <a href="https://topepo.github.io/caret/index.html"><code>caret</code> R package</a>:</p>
<pre class="r"><code>trn_indx &lt;- createDataPartition(iris$Species, p = .8, list = FALSE,
                                times = 1) %&gt;%
  as.numeric()
tst_indx &lt;- which(!(seq_len(nrow(iris)) %in% trn_indx))</code></pre>
<p>Now that we have our training and testing splits, we can organize the data into tasks – the central bookkeeping object in the <code>sl3</code> framework. Essentially, tasks represent a, well, data analytic <em>task</em> that is to be solved by invoking the various machine learning algorithms made available by <code>sl3</code>.</p>
<pre class="r"><code># a task with the data from the training split
iris_task_train &lt;- sl3_Task$new(
  data = iris[trn_indx, ],
  covariates = colnames(iris)[-5],
  outcome = colnames(iris)[5],
  outcome_type = &quot;categorical&quot;
)
iris_task_train</code></pre>
<pre><code>## A sl3 Task with 120 obs and these nodes:
## $covariates
## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## 
## $outcome
## [1] &quot;Species&quot;
## 
## $id
## NULL
## 
## $weights
## NULL
## 
## $offset
## NULL</code></pre>
<pre class="r"><code># a task with the data from the testing split
iris_task_test &lt;- sl3_Task$new(
  data = iris[tst_indx, ],
  covariates = colnames(iris)[-5],
  outcome = colnames(iris)[5],
  outcome_type = &quot;categorical&quot;
)
iris_task_test</code></pre>
<pre><code>## A sl3 Task with 30 obs and these nodes:
## $covariates
## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## 
## $outcome
## [1] &quot;Species&quot;
## 
## $id
## NULL
## 
## $weights
## NULL
## 
## $offset
## NULL</code></pre>
<p>Having set up the data properly, let’s proceed to design <em>pipelines</em> that we can rely on for processing and analyzing the data. A <strong>pipeline</strong> simply represents a set of machine learning procedures to be invoked sequentially, with the results derived from earlier algorithms in the pipeline being used to train those later in the pipeline. Thus, a pipeline is a closed <em>end-to-end</em> system for resolving the problem posed by an <code>sl3</code> task.</p>
<p>We’ll rely on PCA for dimension reduction, gathering only the two most important principal component dimensions to use in training our classification models. Since this is a quick experiment with a well-studied data set, we’ll use just two classification procedures: (1) Logistic regression with regularization (e.g., the LASSO) and (2) Random Forests.</p>
<pre class="r"><code>pca_h2o &lt;- Lrnr_h2o_mutator$new(algorithm = &quot;pca&quot;, k = 2, impute_missing = TRUE)
glmnet_learner &lt;- Lrnr_glmnet$new()
rf_learner &lt;- Lrnr_randomForest$new()</code></pre>
<p>Above, we merely instantiate the learners by invoking the <code>$new()</code> method of each of the appropriate objects. We now have a PCA method that generates and extracts just the first two principal components derived from the design matrix.</p>
<p><strong>Aside on H2O</strong>: Here, we rely on the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/pca.html">implementation of PCA by H2O.ai</a>; this implementation takes advantage of the H2O data science platform and is available for R via the <a href="https://cran.r-project.org/web/packages/h2o/index.html"><code>h2o</code> package</a>. Since we’ll be using H2O, we need to set up an appropriate cluster on our local machine with</p>
<pre class="r"><code>library(h2o)
h2o.init()</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/sr/8wdg8m6s5pv211sp22lr5dlw0000gn/T//RtmpdhjVZa/h2o_nimahejazi_started_from_r.out
##     /var/folders/sr/8wdg8m6s5pv211sp22lr5dlw0000gn/T//RtmpdhjVZa/h2o_nimahejazi_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: .... Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         4 seconds 115 milliseconds 
##     H2O cluster version:        3.16.0.2 
##     H2O cluster version age:    1 month and 25 days  
##     H2O cluster name:           H2O_started_from_R_nimahejazi_efh185 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   1.78 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.4.3 (2017-11-30)</code></pre>
<p>Other than our PCA learner, we’ve also instantiated a logistic regression model (<code>fglm_learner</code> above) based on the fast implementation available through the <a href="https://cran.r-project.org/package=speedglm"><code>speedglm</code> R package</a>, as well as a random forest model based on the canonical implementation available in the <a href="https://cran.r-project.org/package=randomForest"><code>randomForest</code> R package</a>.</p>
<p>Now that our individual learners are set up, we can intuitively string them into pipelines like so</p>
<pre class="r"><code>pca_to_glmnet &lt;- Pipeline$new(pca_h2o, glmnet_learner)
pca_to_rf &lt;- Pipeline$new(pca_h2o, rf_learner)</code></pre>
<p>The first pipeline above merely invokes our PCA learner, extracting the first two principal components of the design matrix from the input task and passing these as inputs to the logistic regression model. Similarly, the second pipeline invokes PCA and passes the results to our random forest model.</p>
<p>To streamline the training of our pipelines, we’ll bundle them into a single <em>stack</em>, then train the model stack all at once. Similar in spirit to a pipeline, a stack is a bundle of <code>sl3</code> learner objects that are to be trained together. The principle difference is that learners in a pipeline are trained sequentially, as described above, while those in a stack are trained in parallel (not in the computational sense, though we can, of course, speed up the fitting procedure with parallelization). Thus, the models in a stack are trained independently of one another.</p>
<p>Now, let’s go ahead a generate a stack and train the two pipelines on our training split of the iris dataset:</p>
<pre class="r"><code>model_stack &lt;- Stack$new(pca_to_glmnet, pca_to_rf)
fit_model_stack &lt;- model_stack$train(iris_task_train)</code></pre>
<pre class="r"><code>out_model_stack &lt;- fit_model_stack$predict(iris_task_test)
pipe1_preds &lt;- as.data.table(matrix(unlist(out_model_stack[[1]]), ncol = 3))
pipe2_preds &lt;- as.data.table(matrix(unlist(out_model_stack[[2]]), ncol = 3))</code></pre>
<p>After extracting the predicted probabilities of each observation being in a given class (the iris species), we now clean up the results a bit to make them more report-able</p>
<pre class="r"><code>outcome_names &lt;- c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)
setnames(pipe1_preds, outcome_names)
setnames(pipe2_preds, outcome_names)

# get class predictions
(pipe1_classes &lt;- outcome_names[apply(pipe1_preds, 1, which.max)])</code></pre>
<pre><code>##  [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot;
##  [6] &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;    
## [11] &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot; 
## [16] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot;
## [21] &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;    
## [26] &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<pre class="r"><code>(pipe2_classes &lt;- outcome_names[apply(pipe2_preds, 1, which.max)])</code></pre>
<pre><code>##  [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot;
##  [6] &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;    
## [11] &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot; 
## [16] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot;
## [21] &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;    
## [26] &quot;versicolor&quot; &quot;virginica&quot;  &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<!--
Something is very wrong with how I'm doing this since the results don't seem
even a little sensible.
-->
